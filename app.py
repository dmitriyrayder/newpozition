import streamlit as st
import pandas as pd
import numpy as np
from catboost import CatBoostRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
import optuna
import logging
import warnings
import plotly.express as px
from datetime import datetime, timedelta

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
warnings.filterwarnings('ignore')

st.set_page_config(page_title="–ú–æ–¥–Ω—ã–π –°–æ–≤–µ—Ç–Ω–∏–∫", layout="wide")

# –°—Ç–∏–ª–∏
st.markdown("""
<style>
.main { background-color: #fce4ec; }
h1 { font-family: 'Comic Sans MS', cursive, sans-serif; color: #e91e63; text-align: center; }
.stButton>button {
    color: white; background: linear-gradient(to right, #f06292, #e91e63); 
    border-radius: 25px; border: 2px solid #ad1457; padding: 12px 28px; font-weight: bold;
}
.metric-card { padding: 10px; border-radius: 10px; background-color: #fff1f8; border: 1px solid #f8bbd0; }
</style>
""", unsafe_allow_html=True)

st.title("üíñ –ú–æ–¥–Ω—ã–π –°–æ–≤–µ—Ç–Ω–∏–∫ –ø–æ –ü—Ä–æ–¥–∞–∂–∞–º")

# –§—É–Ω–∫—Ü–∏–∏
@st.cache_data
def load_data(file):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    try:
        if file.size > 50 * 1024 * 1024:
            st.error("–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π! –ú–∞–∫—Å–∏–º—É–º 50MB")
            return None
            
        if file.name.endswith('.csv'):
            for encoding in ['utf-8', 'cp1251', 'latin1']:
                try:
                    df = pd.read_csv(file, encoding=encoding)
                    logger.info(f"CSV –∑–∞–≥—Ä—É–∂–µ–Ω —Å {encoding}")
                    return df
                except UnicodeDecodeError:
                    continue
            st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–¥–∏—Ä–æ–≤–∫—É")
            return None
        else:
            df = pd.read_excel(file)
            logger.info("Excel —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω")
            return df
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {str(e)}")
        return None

def parse_dates_robust(df, date_col):
    """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç"""
    df = df.copy()
    original_count = len(df)
    
    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
    if date_col in df.columns:
        # –°–Ω–∞—á–∞–ª–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
        
        # –ï—Å–ª–∏ –º–Ω–æ–≥–æ NaT, –ø—Ä–æ–±—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
        if df[date_col].isna().sum() > len(df) * 0.1:
            st.warning("‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º—ã —Å –¥–∞—Ç–∞–º–∏, –ø—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ")
            
            # –ü–æ–ø—ã—Ç–∫–∞ —Å dtype
            try:
                df[date_col] = pd.to_datetime(df[date_col], format='mixed', errors='coerce')
            except:
                # –§–æ—Ä–º–∞—Ç—ã –¥–ª—è –ø–æ–ø—ã—Ç–∫–∏
                formats = ['%Y-%m-%d', '%d.%m.%Y', '%d/%m/%Y', '%m/%d/%Y', '%d-%m-%Y']
                for fmt in formats:
                    mask = df[date_col].isna()
                    if mask.sum() == 0:
                        break
                    try:
                        df.loc[mask, date_col] = pd.to_datetime(
                            df.loc[mask, date_col].astype(str), format=fmt, errors='coerce'
                        )
                    except:
                        continue
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–∞—Ç–∞–º
    bad_dates = df[date_col].isna().sum()
    bad_dates_pct = (bad_dates / original_count) * 100 if original_count > 0 else 0
    
    return df, bad_dates, bad_dates_pct

@st.cache_data
def process_data(_df, art_col, magazin_col, date_col, qty_col, price_col, cat_features):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"""
    df = _df.copy()
    initial_rows = len(df)
    
    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫
    rename_map = {art_col: 'Art', magazin_col: 'Magazin', date_col: 'date', 
                  qty_col: 'Qty', price_col: 'Price'}
    df.rename(columns=rename_map, inplace=True)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç —Å –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ–º
    df, bad_dates, bad_dates_pct = parse_dates_robust(df, 'date')
    
    # –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df.dropna(subset=['date', 'Qty', 'Art', 'Magazin', 'Price'], inplace=True)
    df = df[(df['Qty'] > 0) & (df['Price'] > 0)]
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ (IQR –º–µ—Ç–æ–¥)
    for col in ['Qty', 'Price']:
        Q1, Q3 = df[col].quantile([0.25, 0.75])
        IQR = Q3 - Q1
        df = df[(df[col] >= Q1 - 1.5*IQR) & (df[col] <= Q3 + 1.5*IQR)]
    
    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –∑–∞ –ø–µ—Ä–≤—ã–µ 30 –¥–Ω–µ–π
    df = df.sort_values(['Art', 'Magazin', 'date'])
    first_sales = df.groupby(['Art', 'Magazin'])['date'].first().reset_index()
    first_sales.rename(columns={'date': 'first_date'}, inplace=True)
    
    df = pd.merge(df, first_sales, on=['Art', 'Magazin'])
    df = df[df['date'] <= (df['first_date'] + pd.Timedelta(days=30))]
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    df['days_since_launch'] = (df['date'] - df['first_date']).dt.days
    df['revenue'] = df['Qty'] * df['Price']
    
    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è
    agg_dict = {
        'Qty': ['sum', 'mean', 'std'],
        'Price': ['mean', 'std'],
        'revenue': 'sum',
        'days_since_launch': 'max'
    }
    
    for cat_col in cat_features:
        if cat_col in df.columns:
            agg_dict[cat_col] = 'first'
    
    df_agg = df.groupby(['Art', 'Magazin']).agg(agg_dict).reset_index()
    df_agg.columns = ['_'.join(col).strip() if col[1] else col[0] for col in df_agg.columns]
    
    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫
    rename_dict = {
        'Qty_sum': 'Qty_30_days', 'Qty_mean': 'Avg_daily_qty', 'Qty_std': 'Qty_volatility',
        'Price_mean': 'Price', 'Price_std': 'Price_volatility',
        'revenue_sum': 'Total_revenue', 'days_since_launch_max': 'Days_in_sale'
    }
    df_agg.rename(columns=rename_dict, inplace=True)
    df_agg['Qty_volatility'] = df_agg['Qty_volatility'].fillna(0)
    df_agg['Price_volatility'] = df_agg['Price_volatility'].fillna(0)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    stats = {
        "initial_rows": initial_rows,
        "final_rows": len(df_agg),
        "bad_dates": bad_dates,
        "bad_dates_pct": bad_dates_pct,
        "removed_rows": initial_rows - len(df_agg),
        "removed_pct": ((initial_rows - len(df_agg)) / initial_rows * 100) if initial_rows > 0 else 0,
        "unique_products": df_agg['Art'].nunique(),
        "unique_stores": df_agg['Magazin'].nunique()
    }
    
    return df_agg, stats

@st.cache_resource
def train_model(_df_agg, cat_features, n_trials=30):
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π"""
    target = 'Qty_30_days'
    base_features = ['Magazin', 'Price', 'Avg_daily_qty', 'Price_volatility', 'Days_in_sale']
    features = base_features + list(cat_features)
    available_features = [f for f in features if f in _df_agg.columns]
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df_processed = _df_agg[available_features + [target]].copy()
    cat_features_list = ['Magazin'] + [f for f in cat_features if f in df_processed.columns]
    
    for col in cat_features_list:
        if col in df_processed.columns:
            df_processed[col] = df_processed[col].astype(str)
    
    X, y = df_processed[available_features], df_processed[target]
    test_size = min(0.25, max(0.1, 20 / len(X)))
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    def objective(trial):
        params = {
            'iterations': trial.suggest_int('iterations', 300, 800),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
            'depth': trial.suggest_int('depth', 4, 8),
            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),
            'verbose': False, 'random_seed': 42
        }
        
        try:
            model = CatBoostRegressor(**params)
            model.fit(X_train, y_train, cat_features=cat_features_list, 
                     eval_set=(X_test, y_test), early_stopping_rounds=50, 
                     use_best_model=True, verbose=False)
            return mean_absolute_error(y_test, model.predict(X_test))
        except:
            return float('inf')
    
    # –û–±—É—á–µ–Ω–∏–µ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
    progress_bar = st.progress(0)
    optuna.logging.set_verbosity(optuna.logging.WARNING)
    study = optuna.create_study(direction='minimize')
    
    for i in range(n_trials):
        study.optimize(objective, n_trials=1)
        progress_bar.progress((i + 1) / n_trials)
    
    progress_bar.empty()
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å
    best_params = study.best_params
    best_params.update({'verbose': False, 'random_seed': 42})
    
    final_model = CatBoostRegressor(**best_params)
    final_model.fit(X, y, cat_features=cat_features_list, verbose=False)
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    test_preds = final_model.predict(X_test)
    train_preds = final_model.predict(X_train)
    
    metrics = {
        'MAE': mean_absolute_error(y_test, test_preds),
        'RMSE': np.sqrt(mean_squared_error(y_test, test_preds)),
        'R2': r2_score(y_test, test_preds),
        'overfit_ratio': mean_absolute_error(y_train, train_preds) / mean_absolute_error(y_test, test_preds),
        'feature_importance': dict(zip(available_features, final_model.feature_importances_))
    }
    
    return final_model, available_features, metrics

def create_prediction_form(cat_features, df_agg):
    """–§–æ—Ä–º–∞ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è"""
    st.subheader("‚úçÔ∏è –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏")
    
    with st.form("prediction_form"):
        data = {}
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Ç–æ–≤–∞—Ä–∞:**")
            for feature in cat_features:
                if feature in df_agg.columns:
                    top_values = df_agg[feature].value_counts().head(3).index.tolist()
                    data[feature] = st.text_input(
                        f"{feature} ‚ú®", 
                        placeholder=f"–ù–∞–ø—Ä–∏–º–µ—Ä: {top_values[0]}" if top_values else "–í–≤–µ–¥–∏—Ç–µ –∑–Ω–∞—á–µ–Ω–∏–µ"
                    )
        
        with col2:
            st.markdown("**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø—Ä–æ–¥–∞–∂:**")
            if 'Price' in df_agg.columns:
                price_stats = df_agg['Price'].describe()
                data['Price'] = st.number_input(
                    "–¶–µ–Ω–∞ üí∞", min_value=1.0, value=float(price_stats['mean']), 
                    step=1.0, format="%.2f"
                )
            
            data['Avg_daily_qty'] = st.number_input(
                "–û–∂–∏–¥–∞–µ–º—ã–µ –ø—Ä–æ–¥–∞–∂–∏/–¥–µ–Ω—å", min_value=0.1, value=1.0, step=0.1
            )
            data['Days_in_sale'] = st.slider("–î–Ω–∏ –≤ –ø—Ä–æ–¥–∞–∂–µ", 1, 30, 30)
            data['Price_volatility'] = st.number_input("–í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å —Ü–µ–Ω—ã", 0.0, value=0.0, step=0.1)
        
        return st.form_submit_button("üîÆ –°–æ–∑–¥–∞—Ç—å –ø—Ä–æ–≥–Ω–æ–∑!", type="primary"), data

def make_predictions(model, features, product_data, df_agg):
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤"""
    # –í–∞–ª–∏–¥–∞—Ü–∏—è
    for key, value in product_data.items():
        if key not in ['Price', 'Avg_daily_qty', 'Days_in_sale', 'Price_volatility']:
            if pd.isna(value) or str(value).strip() == "":
                st.error(f"–ü–æ–ª–µ '{key}' –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º!")
                return pd.DataFrame()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞
    stores = df_agg['Magazin'].unique()
    predictions_data = []
    
    for store in stores:
        row = product_data.copy()
        row['Magazin'] = store
        predictions_data.append(row)
    
    pred_df = pd.DataFrame(predictions_data)
    available_features = [f for f in features if f in pred_df.columns]
    
    try:
        # –ü—Ä–æ–≥–Ω–æ–∑—ã
        raw_preds = model.predict(pred_df[available_features])
        pred_df['–ü—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–∞–∂ (30 –¥–Ω–µ–π, —à—Ç.)'] = np.maximum(0, np.round(raw_preds, 0))
        
        # –î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
        pred_std = raw_preds.std()
        pred_df['–ú–∏–Ω. –ø—Ä–æ–≥–Ω–æ–∑'] = np.maximum(0, np.round(raw_preds - 1.96 * pred_std, 0))
        pred_df['–ú–∞–∫—Å. –ø—Ä–æ–≥–Ω–æ–∑'] = np.round(raw_preds + 1.96 * pred_std, 0)
        
        # –†–µ–π—Ç–∏–Ω–≥
        max_pred = pred_df['–ü—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–∞–∂ (30 –¥–Ω–µ–π, —à—Ç.)'].max()
        pred_df['–†–µ–π—Ç–∏–Ω–≥ —É—Å–ø–µ—Ö–∞ (%)'] = np.round(
            (pred_df['–ü—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–∞–∂ (30 –¥–Ω–µ–π, —à—Ç.)'] / max_pred * 100) if max_pred > 0 else 0, 0
        )
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏
        def categorize(rating):
            if rating >= 80: return "üî• –•–∏—Ç –ø—Ä–æ–¥–∞–∂"
            elif rating >= 60: return "‚≠ê –•–æ—Ä–æ—à–∏–µ –ø—Ä–æ–¥–∞–∂–∏"
            elif rating >= 40: return "üìà –°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ–¥–∞–∂–∏"
            else: return "üîß –¢—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è"
        
        pred_df['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'] = pred_df['–†–µ–π—Ç–∏–Ω–≥ —É—Å–ø–µ—Ö–∞ (%)'].apply(categorize)
        
        # –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –≤—ã—Ä—É—á–∫–∞
        if 'Price' in product_data:
            pred_df['–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –≤—ã—Ä—É—á–∫–∞'] = pred_df['–ü—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–∞–∂ (30 –¥–Ω–µ–π, —à—Ç.)'] * product_data['Price']
        
        # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
        pred_df.rename(columns={'Magazin': '–ë—É—Ç–∏–∫'}, inplace=True)
        return pred_df.sort_values('–ü—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–∞–∂ (30 –¥–Ω–µ–π, —à—Ç.)', ascending=False)
        
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è: {str(e)}")
        return pd.DataFrame()

def create_visualizations(predictions_df):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏"""
    if predictions_df.empty:
        return
    
    col1, col2 = st.columns(2)
    
    with col1:
        # –¢–æ–ø-10 –º–∞–≥–∞–∑–∏–Ω–æ–≤
        top_10 = predictions_df.head(10)
        fig1 = px.bar(top_10, x='–ü—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–∞–∂ (30 –¥–Ω–µ–π, —à—Ç.)', y='–ë—É—Ç–∏–∫', 
                     orientation='h', title="–¢–æ–ø-10 –º–∞–≥–∞–∑–∏–Ω–æ–≤", 
                     color='–†–µ–π—Ç–∏–Ω–≥ —É—Å–ø–µ—Ö–∞ (%)', color_continuous_scale='viridis')
        fig1.update_layout(height=400)
        st.plotly_chart(fig1, use_container_width=True)
    
    with col2:
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        category_counts = predictions_df['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'].value_counts()
        fig2 = px.pie(values=category_counts.values, names=category_counts.index,
                     title="–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º")
        fig2.update_layout(height=400)
        st.plotly_chart(fig2, use_container_width=True)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
if 'processed' not in st.session_state:
    st.session_state.processed = False

# –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
dataset_file = st.file_uploader("üíñ –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏", type=["csv", "xlsx", "xls"])

if dataset_file:
    df_raw = load_data(dataset_file)
    
    if df_raw is not None:
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ
        with st.expander("üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ"):
            col1, col2, col3 = st.columns(3)
            with col1: st.metric("–°—Ç—Ä–æ–∫", len(df_raw))
            with col2: st.metric("–ö–æ–ª–æ–Ω–æ–∫", len(df_raw.columns))
            with col3: st.metric("–†–∞–∑–º–µ—Ä (MB)", f"{dataset_file.size / (1024*1024):.2f}")
            st.dataframe(df_raw.head(), use_container_width=True)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–ª–æ–Ω–æ–∫
        st.subheader("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
        
        col1, col2 = st.columns([3, 1])
        with col1:
            cols = st.columns(5)
            with cols[0]: art_col = st.selectbox("–ê—Ä—Ç–∏–∫—É–ª", df_raw.columns, index=0)
            with cols[1]: magazin_col = st.selectbox("–ú–∞–≥–∞–∑–∏–Ω", df_raw.columns, index=min(1, len(df_raw.columns)-1))
            with cols[2]: date_col = st.selectbox("–î–∞—Ç–∞", df_raw.columns, index=min(2, len(df_raw.columns)-1))
            with cols[3]: qty_col = st.selectbox("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ", df_raw.columns, index=min(3, len(df_raw.columns)-1))
            with cols[4]: price_col = st.selectbox("–¶–µ–Ω–∞", df_raw.columns, index=min(4, len(df_raw.columns)-1))
        
        with col2:
            required_cols = [art_col, magazin_col, date_col, qty_col, price_col]
            available_cols = [col for col in df_raw.columns if col not in required_cols]
            cat_features = st.multiselect("–î–æ–ø. –ø—Ä–∏–∑–Ω–∞–∫–∏", available_cols)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        if st.button("üöÄ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∏ –æ–±—É—á–∏—Ç—å", type="primary"):
            if len(df_raw) < 10:
                st.error("–°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö (–º–∏–Ω–∏–º—É–º 10 –∑–∞–ø–∏—Å–µ–π)")
            else:
                with st.spinner("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ..."):
                    df_agg, stats = process_data(df_raw, art_col, magazin_col, date_col, 
                                               qty_col, price_col, cat_features)
                    
                    if len(df_agg) > 0:
                        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
                        st.success("‚úÖ –î–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!")
                        
                        col1, col2, col3, col4 = st.columns(4)
                        with col1: st.metric("–ó–∞–ø–∏—Å–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è", stats['final_rows'])
                        with col2: st.metric("–£–¥–∞–ª–µ–Ω–æ —Å—Ç—Ä–æ–∫", f"{stats['removed_rows']} ({stats['removed_pct']:.1f}%)")
                        with col3: st.metric("–ü–ª–æ—Ö–∏–µ –¥–∞—Ç—ã", f"{stats['bad_dates']} ({stats['bad_dates_pct']:.1f}%)")
                        with col4: st.metric("–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤", stats['unique_products'])
                        
                        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
                        with st.spinner("–û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å..."):
                            model, features, metrics = train_model(df_agg, cat_features)
                            
                            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Å–µ—Å—Å–∏—é
                            st.session_state.update({
                                'df_agg': df_agg, 'model': model, 'features': features,
                                'metrics': metrics, 'cat_features': cat_features, 'processed': True
                            })
                            
                            st.success("üéØ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞!")
                            col1, col2 = st.columns(2)
                            with col1: st.metric("MAE", f"{metrics['MAE']:.2f}")
                            with col2: st.metric("R¬≤", f"{metrics['R2']:.3f}")
                    else:
                        st.error("–ù–µ –æ—Å—Ç–∞–ª–æ—Å—å –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏!")

# –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ
if st.session_state.get('processed', False):
    st.divider()
    
    # –§–æ—Ä–º–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è
    submitted, product_data = create_prediction_form(
        st.session_state.cat_features, st.session_state.df_agg
    )
    
    if submitted:
        predictions_df = make_predictions(
            st.session_state.model, st.session_state.features, 
            product_data, st.session_state.df_agg
        )
        
        if not predictions_df.empty:
            st.success("üéâ –ü—Ä–æ–≥–Ω–æ–∑ –≥–æ—Ç–æ–≤!")
            
            # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è - –ò–°–ü–†–ê–í–õ–ï–ù–ê –õ–û–ì–ò–ö–ê
            st.subheader("üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è")
            
            # –¢–æ–ø-3 —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –∫–æ–ª–æ–Ω–æ–∫
            top_3 = predictions_df.head(3)
            medals = ["ü•á", "ü•à", "ü•â"]
            
            cols = st.columns(3)
            for i, (_, row) in enumerate(top_3.iterrows()):
                with cols[i]:
                    st.metric(
                        f"{medals[i]} {row['–ë—É—Ç–∏–∫']}",
                        f"{int(row['–ü—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–∞–∂ (30 –¥–Ω–µ–π, —à—Ç.)'])} —à—Ç.",
                        f"{int(row['–†–µ–π—Ç–∏–Ω–≥ —É—Å–ø–µ—Ö–∞ (%)'])}%"
                    )
            
            # –ü–æ–ª–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
            display_cols = ['–ë—É—Ç–∏–∫', '–ü—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–∞–∂ (30 –¥–Ω–µ–π, —à—Ç.)', '–†–µ–π—Ç–∏–Ω–≥ —É—Å–ø–µ—Ö–∞ (%)', 
                          '–ö–∞—Ç–µ–≥–æ—Ä–∏—è', '–ú–∏–Ω. –ø—Ä–æ–≥–Ω–æ–∑', '–ú–∞–∫—Å. –ø—Ä–æ–≥–Ω–æ–∑']
            if '–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –≤—ã—Ä—É—á–∫–∞' in predictions_df.columns:
                display_cols.append('–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –≤—ã—Ä—É—á–∫–∞')
            
            st.dataframe(predictions_df[display_cols], use_container_width=True, hide_index=True)
            
            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
            create_visualizations(predictions_df)
            
            # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ
            csv = predictions_df.to_csv(index=False)
            st.download_button("üíæ –°–∫–∞—á–∞—Ç—å –ø—Ä–æ–≥–Ω–æ–∑", csv, "forecast.csv", "text/csv")

# –°–ø—Ä–∞–≤–∫–∞
with st.sidebar:
    st.header("‚ÑπÔ∏è –°–ø—Ä–∞–≤–∫–∞")
    st.markdown("""
    **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:**
    1. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª (CSV/Excel)
    2. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –∫–æ–ª–æ–Ω–∫–∏
    3. –û–±—Ä–∞–±–æ—Ç–∞–π—Ç–µ –¥–∞–Ω–Ω—ã–µ
    4. –°–æ–∑–¥–∞–π—Ç–µ –ø—Ä–æ–≥–Ω–æ–∑
    
    **–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:**
    - –ú–∏–Ω–∏–º—É–º 10 –∑–∞–ø–∏—Å–µ–π
    - –ö–æ–ª–æ–Ω–∫–∏: –∞—Ä—Ç–∏–∫—É–ª, –º–∞–≥–∞–∑–∏–Ω, –¥–∞—Ç–∞, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ, —Ü–µ–Ω–∞
    """)
    
    if st.session_state.get('processed', False):
        st.success("‚úÖ –ì–æ—Ç–æ–≤–æ –∫ –ø—Ä–æ–≥–Ω–æ–∑–∞–º!")
        st.info(f"R¬≤ = {st.session_state.metrics['R2']:.3f}")
