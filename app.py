import streamlit as st
import pandas as pd
import numpy as np
from catboost import CatBoostRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
import optuna
import logging
from typing import Dict, List, Tuple, Optional
import warnings
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
warnings.filterwarnings('ignore')

# --- –°–¢–ò–õ–ò–ó–ê–¶–ò–Ø –ò–ù–¢–ï–†–§–ï–ô–°–ê ---
st.set_page_config(page_title="–ú–æ–¥–Ω—ã–π –°–æ–≤–µ—Ç–Ω–∏–∫", layout="wide", initial_sidebar_state="collapsed")

st.markdown("""
<style>
.main { background-color: #fce4ec; }
h1 { font-family: 'Comic Sans MS', cursive, sans-serif; color: #e91e63; text-align: center; text-shadow: 2px 2px 4px #f8bbd0; }
h2, h3 { font-family: 'Comic Sans MS', cursive, sans-serif; color: #ad1457; }
.stButton>button {
    color: white; background-image: linear-gradient(to right, #f06292, #e91e63); border-radius: 25px;
    border: 2px solid #ad1457; padding: 12px 28px; font-weight: bold; font-size: 18px;
    box-shadow: 0 4px 15px 0 rgba(233, 30, 99, 0.4); transition: all 0.3s ease 0s;
}
.stButton>button:hover { background-position: right center; box-shadow: 0 4px 15px 0 rgba(233, 30, 99, 0.75); }
.stExpander { border: 2px solid #f8bbd0; border-radius: 10px; background-color: #fff1f8; }
.metric-card { padding: 10px; border-radius: 10px; background-color: #fff1f8; border: 1px solid #f8bbd0; }
.prediction-card { 
    background: linear-gradient(135deg, #fff1f8 0%, #fce4ec 100%);
    padding: 20px; border-radius: 15px; border: 2px solid #f8bbd0;
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
}
</style>
""", unsafe_allow_html=True)

st.title("üíñ –ú–æ–¥–Ω—ã–π –°–æ–≤–µ—Ç–Ω–∏–∫ –ø–æ –ü—Ä–æ–¥–∞–∂–∞–º üíñ")

# --- –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ---

def validate_dataframe(df: pd.DataFrame, required_columns: List[str]) -> bool:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    if df.empty:
        st.error("–§–∞–π–ª –ø—É—Å—Ç! –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏.")
        return False
    
    missing_cols = [col for col in required_columns if col not in df.columns]
    if missing_cols:
        st.error(f"–í —Ñ–∞–π–ª–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing_cols}")
        return False
    
    if len(df) < 10:
        st.warning("–°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (–º–∏–Ω–∏–º—É–º 10 –∑–∞–ø–∏—Å–µ–π)")
        return False
    
    return True

def safe_index_selection(columns, default_index: int = 0) -> int:
    """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –≤—ã–±–æ—Ä –∏–Ω–¥–µ–∫—Å–∞ –∫–æ–ª–æ–Ω–∫–∏"""
    if len(columns) == 0:
        return 0
    return min(default_index, len(columns) - 1)

@st.cache_data
def load_data(file) -> Optional[pd.DataFrame]:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –¥–∞—Ç"""
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞ (–º–∞–∫—Å–∏–º—É–º 50MB)
        if hasattr(file, 'size') and file.size > 50 * 1024 * 1024:
            st.error("–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π! –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: 50MB")
            return None
            
        if file.name.endswith('.csv'):
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
            for encoding in ['utf-8', 'cp1251', 'latin1', 'utf-8-sig']:
                try:
                    df = pd.read_csv(file, encoding=encoding)
                    logger.info(f"CSV —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π {encoding}")
                    return df
                except UnicodeDecodeError:
                    continue
            st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–¥–∏—Ä–æ–≤–∫—É CSV —Ñ–∞–π–ª–∞")
            return None
        else:
            # –î–ª—è Excel —Ñ–∞–π–ª–æ–≤ —Å –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–º —Ç–∏–ø–æ–º –¥–∞—Ç—ã
            try:
                df = pd.read_excel(file, engine='openpyxl')
                logger.info("Excel —Ñ–∞–π–ª —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω")
                return df
            except Exception as e:
                # –ü–æ–ø—ã—Ç–∫–∞ —Å –¥—Ä—É–≥–∏–º–∏ –¥–≤–∏–∂–∫–∞–º–∏
                try:
                    df = pd.read_excel(file, engine='xlrd')
                    logger.info("Excel —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω —Å xlrd")
                    return df
                except:
                    raise e
            
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞: {str(e)}")
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞: {e}")
        return None

def parse_dates_robust(df: pd.DataFrame, date_col: str) -> pd.DataFrame:
    """–†–æ–±—É—Å—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç —Å –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ–º"""
    df = df.copy()
    
    # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
    
    # –ï—Å–ª–∏ –º–Ω–æ–≥–æ NaT, –ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
    if df[date_col].isna().sum() > len(df) * 0.1:  # –ï—Å–ª–∏ >10% –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–ª–∏—Å—å
        st.warning("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã —Å —Ñ–æ—Ä–º–∞—Ç–æ–º –¥–∞—Ç. –ü—Ä–∏–º–µ–Ω—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É...")
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
        date_formats = [
            '%Y-%m-%d', '%d.%m.%Y', '%d/%m/%Y', '%m/%d/%Y',
            '%Y-%m-%d %H:%M:%S', '%d.%m.%Y %H:%M:%S',
            '%d-%m-%Y', '%Y/%m/%d'
        ]
        
        original_col = f"{date_col}_original"
        df[original_col] = df[date_col]
        
        for fmt in date_formats:
            mask = df[date_col].isna()
            if mask.sum() == 0:
                break
            try:
                df.loc[mask, date_col] = pd.to_datetime(
                    df.loc[mask, original_col], format=fmt, errors='coerce'
                )
            except:
                continue
        
        df.drop(columns=[original_col], inplace=True)
    
    return df

@st.cache_data
def process_and_aggregate(
    _df: pd.DataFrame, 
    art_col: str, 
    magazin_col: str, 
    date_col: str, 
    qty_col: str, 
    price_col: str, 
    cat_features: Tuple[str, ...]
) -> Tuple[pd.DataFrame, Dict]:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"""
    
    df = _df.copy()
    
    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫
    column_map = {
        art_col: 'Art', 
        magazin_col: 'Magazin', 
        date_col: 'date', 
        qty_col: 'Qty', 
        price_col: 'Price'
    }
    df.rename(columns=column_map, inplace=True)
    
    initial_rows = len(df)
    
    # –†–æ–±–∞—Å—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç
    df = parse_dates_robust(df, 'date')
    bad_date_rows = df['date'].isna().sum()
    df.dropna(subset=['date'], inplace=True)
    
    # –û—á–∏—Å—Ç–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
    crucial_cols = ['Qty', 'Art', 'Magazin', 'Price']
    df.dropna(subset=crucial_cols, inplace=True)
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    df = df[df['Qty'] > 0]  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º
    df = df[df['Price'] > 0]  # –¶–µ–Ω–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–π
    
    # –£–ª—É—á—à–µ–Ω–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º IQR
    def remove_outliers_iqr(data, column):
        Q1 = data[column].quantile(0.25)
        Q3 = data[column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        return data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]
    
    df = remove_outliers_iqr(df, 'Qty')
    df = remove_outliers_iqr(df, 'Price')
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∏ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞
    df = df.sort_values(by=['Art', 'Magazin', 'date'])
    
    # –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–≤—É—é –¥–∞—Ç—É –ø—Ä–æ–¥–∞–∂–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã —Ç–æ–≤–∞—Ä-–º–∞–≥–∞–∑–∏–Ω
    first_sale_dates = df.groupby(['Art', 'Magazin'])['date'].first().reset_index()
    first_sale_dates.rename(columns={'date': 'first_sale_date'}, inplace=True)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    df_merged = pd.merge(df, first_sale_dates, on=['Art', 'Magazin'])
    
    # –ë–µ—Ä–µ–º –¥–∞–Ω–Ω—ã–µ –∑–∞ –ø–µ—Ä–≤—ã–µ 30 –¥–Ω–µ–π
    df_30_days = df_merged[
        df_merged['date'] <= (df_merged['first_sale_date'] + pd.Timedelta(days=30))
    ].copy()
    
    # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    df_30_days['days_since_launch'] = (df_30_days['date'] - df_30_days['first_sale_date']).dt.days
    df_30_days['revenue'] = df_30_days['Qty'] * df_30_days['Price']
    
    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
    agg_logic = {
        'Qty': ['sum', 'mean', 'std'],
        'Price': ['mean', 'std'],
        'revenue': 'sum',
        'days_since_launch': 'max'
    }
    
    for cat_col in cat_features:
        if cat_col in df_30_days.columns:
            agg_logic[cat_col] = 'first'
    
    df_agg = df_30_days.groupby(['Art', 'Magazin'], as_index=False).agg(agg_logic)
    
    # –°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–ª–æ–Ω–æ–∫
    df_agg.columns = ['_'.join(col).strip() if col[1] else col[0] for col in df_agg.columns.values]
    df_agg.rename(columns={
        'Qty_sum': 'Qty_30_days',
        'Qty_mean': 'Avg_daily_qty',
        'Qty_std': 'Qty_volatility',
        'Price_mean': 'Price',
        'Price_std': 'Price_volatility',
        'revenue_sum': 'Total_revenue_30_days',
        'days_since_launch_max': 'Days_in_sale'
    }, inplace=True)
    
    # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
    df_agg['Qty_volatility'] = df_agg['Qty_volatility'].fillna(0)
    df_agg['Price_volatility'] = df_agg['Price_volatility'].fillna(0)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    stats = {
        "total_rows": initial_rows,
        "final_rows": len(df_agg),
        "bad_date_rows": bad_date_rows,
        "outliers_removed": initial_rows - len(df_30_days),
        "unique_products": df_agg['Art'].nunique(),
        "unique_stores": df_agg['Magazin'].nunique(),
        "date_range": {
            "start": df['date'].min(),
            "end": df['date'].max()
        },
        "avg_price": df_agg['Price'].mean(),
        "total_revenue": df_agg['Total_revenue_30_days'].sum()
    }
    
    logger.info(f"–î–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã: {initial_rows} -> {len(df_agg)} —Å—Ç—Ä–æ–∫")
    
    return df_agg, stats

@st.cache_resource
def train_model_with_optuna(
    _df_agg: pd.DataFrame, 
    cat_features: Tuple[str, ...],
    n_trials: int = 50
) -> Tuple[CatBoostRegressor, List[str], Dict]:
    """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    
    cat_features_list = list(cat_features)
    target = 'Qty_30_days'
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    base_features = ['Magazin', 'Price', 'Avg_daily_qty', 'Price_volatility', 'Days_in_sale']
    features = base_features + cat_features_list
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ –¥–∞–Ω–Ω—ã—Ö
    available_features = [f for f in features if f in _df_agg.columns]
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df_processed = _df_agg[available_features + [target]].copy()
    all_cat_features = ['Magazin'] + cat_features_list
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    for col in all_cat_features:
        if col in df_processed.columns:
            df_processed[col] = df_processed[col].astype(str)
    
    X, y = df_processed[available_features], df_processed[target]
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
    if len(X) < 50:
        st.warning("‚ö†Ô∏è –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è >50 –∑–∞–ø–∏—Å–µ–π)")
    
    # –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    test_size = min(0.25, max(0.1, 20 / len(X)))
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=None
    )
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    def objective(trial):
        params = {
            'iterations': trial.suggest_int('iterations', 300, 1000),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
            'depth': trial.suggest_int('depth', 4, 8),
            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),
            'border_count': trial.suggest_int('border_count', 32, 128),
            'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),
            'random_strength': trial.suggest_float('random_strength', 0, 10),
            'verbose': False,
            'random_seed': 42,
            'loss_function': 'RMSE'
        }
        
        try:
            model = CatBoostRegressor(**params)
            model.fit(
                X_train, y_train, 
                cat_features=[f for f in all_cat_features if f in available_features],
                eval_set=(X_test, y_test),
                early_stopping_rounds=50,
                use_best_model=True,
                verbose=False
            )
            predictions = model.predict(X_test)
            return mean_absolute_error(y_test, predictions)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ objective —Ñ—É–Ω–∫—Ü–∏–∏: {e}")
            return float('inf')
    
    # –û–±—É—á–µ–Ω–∏–µ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    optuna.logging.set_verbosity(optuna.logging.WARNING)
    study = optuna.create_study(direction='minimize')
    
    for i in range(n_trials):
        study.optimize(objective, n_trials=1)
        progress = (i + 1) / n_trials
        progress_bar.progress(progress)
        status_text.text(f"–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: {i+1}/{n_trials} –ø–æ–ø—ã—Ç–æ–∫ (–ª—É—á—à–∏–π MAE: {study.best_value:.2f})")
    
    progress_bar.empty()
    status_text.empty()
    
    # –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    best_params = study.best_params
    best_params['verbose'] = False
    best_params['random_seed'] = 42
    
    final_model = CatBoostRegressor(**best_params)
    final_model.fit(
        X, y, 
        cat_features=[f for f in all_cat_features if f in available_features], 
        verbose=False
    )
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    test_preds = final_model.predict(X_test)
    train_preds = final_model.predict(X_train)
    
    metrics = {
        'MAE_test': mean_absolute_error(y_test, test_preds),
        'MAE_train': mean_absolute_error(y_train, train_preds),
        'RMSE_test': np.sqrt(mean_squared_error(y_test, test_preds)),
        'R2_test': r2_score(y_test, test_preds),
        'R2_train': r2_score(y_train, train_preds),
        'best_params': best_params,
        'feature_importance': dict(zip(available_features, final_model.feature_importances_)),
        'overfit_ratio': mean_absolute_error(y_train, train_preds) / mean_absolute_error(y_test, test_preds)
    }
    
    logger.info(f"–ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞. MAE: {metrics['MAE_test']:.2f}, R¬≤: {metrics['R2_test']:.2f}")
    
    return final_model, available_features, metrics

def create_prediction_form(cat_features: List[str], df_agg: pd.DataFrame) -> Tuple[bool, Dict]:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ–æ—Ä–º–∞ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
    
    st.subheader("‚úçÔ∏è –û–ø–∏—à–∏—Ç–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏")
    
    with st.form("prediction_form"):
        new_product_data = {}
        
        # –°–æ–∑–¥–∞–µ–º –¥–≤–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è
        col1, col2 = st.columns(2)
        
        # –õ–µ–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞ - –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        with col1:
            st.markdown("**–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Ç–æ–≤–∞—Ä–∞:**")
            for feature in cat_features:
                if feature in df_agg.columns:
                    top_values = df_agg[feature].value_counts().head(5).index.tolist()
                    help_text = f"–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ: {', '.join(map(str, top_values[:3]))}" if top_values else None
                    placeholder = f"–ù–∞–ø—Ä–∏–º–µ—Ä: {top_values[0]}" if top_values else "–í–≤–µ–¥–∏—Ç–µ –∑–Ω–∞—á–µ–Ω–∏–µ"
                    
                    new_product_data[feature] = st.text_input(
                        f"{feature} ‚ú®",
                        help=help_text,
                        placeholder=placeholder,
                        key=f"input_{feature}"
                    )
        
        # –ü—Ä–∞–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞ - —á–∏—Å–ª–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        with col2:
            st.markdown("**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø—Ä–æ–¥–∞–∂:**")
            
            # –ü–æ–ª–µ –¥–ª—è —Ü–µ–Ω—ã
            if 'Price' in df_agg.columns:
                price_stats = df_agg['Price'].describe()
                price_mean = float(price_stats['mean'])
                price_min = max(1.0, float(price_stats['min']))
                price_max = float(price_stats['max'])
                
                new_product_data['Price'] = st.number_input(
                    "–¶–µ–Ω–∞ üí∞",
                    min_value=price_min,
                    max_value=price_max * 2,
                    value=price_mean,
                    step=max(1.0, price_mean * 0.05),
                    format="%.2f",
                    help=f"–î–∏–∞–ø–∞–∑–æ–Ω —Ü–µ–Ω –≤ –¥–∞–Ω–Ω—ã—Ö: {price_min:.0f} - {price_max:.0f}"
                )
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞
            if 'Avg_daily_qty' in df_agg.columns:
                avg_daily_mean = float(df_agg['Avg_daily_qty'].mean())
                new_product_data['Avg_daily_qty'] = st.number_input(
                    "–û–∂–∏–¥–∞–µ–º—ã–µ –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–µ –ø—Ä–æ–¥–∞–∂–∏",
                    min_value=0.1,
                    value=avg_daily_mean,
                    step=0.1,
                    format="%.1f",
                    help="–ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–¥–∞–∂ –≤ –¥–µ–Ω—å"
                )
            
            new_product_data['Days_in_sale'] = st.slider(
                "–î–Ω–∏ –≤ –ø—Ä–æ–¥–∞–∂–µ",
                min_value=1,
                max_value=30,
                value=30,
                help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π, –∫–æ—Ç–æ—Ä–æ–µ —Ç–æ–≤–∞—Ä –±—É–¥–µ—Ç –≤ –ø—Ä–æ–¥–∞–∂–µ"
            )
            
            new_product_data['Price_volatility'] = st.number_input(
                "–í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å —Ü–µ–Ω—ã",
                min_value=0.0,
                value=0.0,
                step=0.1,
                help="–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã (0 = —Å—Ç–∞–±–∏–ª—å–Ω–∞—è —Ü–µ–Ω–∞)"
            )
        
        # –ö–Ω–æ–ø–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è
        submitted = st.form_submit_button(
            "üîÆ –°–æ–∑–¥–∞—Ç—å –ø—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–∞–∂!",
            type="primary",
            use_container_width=True
        )
        
        return submitted, new_product_data

def make_predictions(
    model: CatBoostRegressor, 
    features: List[str], 
    new_product_data: Dict, 
    df_agg: pd.DataFrame
) -> pd.DataFrame:
    """–£–ª—É—á—à–µ–Ω–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ —Å –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞–º–∏"""
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    for key, value in new_product_data.items():
        if key not in ['Price', 'Avg_daily_qty', 'Days_in_sale', 'Price_volatility']:
            if pd.isna(value) or str(value).strip() == "":
                st.error(f"‚ö†Ô∏è –ü–æ–ª–µ '{key}' –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º!")
                return pd.DataFrame()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞
    magaziny = df_agg['Magazin'].unique()
    predictions_data = []
    
    for magazin in magaziny:
        row = new_product_data.copy()
        row['Magazin'] = magazin
        predictions_data.append(row)
    
    predictions_df = pd.DataFrame(predictions_data)
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    available_features = [f for f in features if f in predictions_df.columns]
    predictions_df_filtered = predictions_df[available_features]
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤
    try:
        raw_predictions = model.predict(predictions_df_filtered)
        predictions_df['Pred_Qty_30_days'] = np.maximum(0, np.round(raw_predictions, 0))
        
        # –†–∞—Å—á–µ—Ç –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ (–ø—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥)
        predictions_std = raw_predictions.std()
        predictions_df['Pred_Min'] = np.maximum(0, np.round(raw_predictions - 1.96 * predictions_std, 0))
        predictions_df['Pred_Max'] = np.round(raw_predictions + 1.96 * predictions_std, 0)
        
        # –†–∞—Å—á–µ—Ç —Ä–µ–π—Ç–∏–Ω–≥–∞ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        max_pred = predictions_df['Pred_Qty_30_days'].max()
        if max_pred > 0:
            predictions_df['Rating_%'] = np.round(
                (predictions_df['Pred_Qty_30_days'] / max_pred * 100), 0
            )
        else:
            predictions_df['Rating_%'] = 0
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –º–∞–≥–∞–∑–∏–Ω–æ–≤
        def categorize_performance(rating):
            if rating >= 80:
                return "üî• –•–∏—Ç –ø—Ä–æ–¥–∞–∂"
            elif rating >= 60:
                return "‚≠ê –•–æ—Ä–æ—à–∏–µ –ø—Ä–æ–¥–∞–∂–∏"
            elif rating >= 40:
                return "üìà –°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ–¥–∞–∂–∏"
            else:
                return "üîß –¢—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è"
        
        predictions_df['Category'] = predictions_df['Rating_%'].apply(categorize_performance)
        
        # –†–∞—Å—á–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–π –≤—ã—Ä—É—á–∫–∏
        if 'Price' in new_product_data:
            predictions_df['Potential_Revenue'] = predictions_df['Pred_Qty_30_days'] * new_product_data['Price']
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é –ø—Ä–æ–≥–Ω–æ–∑–∞
        result_df = predictions_df.sort_values(
            by='Pred_Qty_30_days', ascending=False
        )
        
        # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        rename_dict = {
            'Magazin': '–ë—É—Ç–∏–∫',
            'Pred_Qty_30_days': '–ü—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–∞–∂ (—à—Ç.)',
            'Rating_%': '–†–µ–π—Ç–∏–Ω–≥ (%)',
            'Category': '–ö–∞—Ç–µ–≥–æ—Ä–∏—è',
            'Pred_Min': '–ú–∏–Ω. –ø—Ä–æ–≥–Ω–æ–∑',
            'Pred_Max': '–ú–∞–∫—Å. –ø—Ä–æ–≥–Ω–æ–∑'
        }
        
        if 'Potential_Revenue' in result_df.columns:
            rename_dict['Potential_Revenue'] = '–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –≤—ã—Ä—É—á–∫–∞'
        
        result_df = result_df.rename(columns=rename_dict)
        
        return result_df
        
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –ø—Ä–æ–≥–Ω–æ–∑–∞: {str(e)}")
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        return pd.DataFrame()

def create_visualizations(predictions_df: pd.DataFrame, metrics: Dict):
    """–°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    
    if predictions_df.empty:
        return
    
    col1, col2 = st.columns(2)
    
    with col1:
        # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ–ø-10 –º–∞–≥–∞–∑–∏–Ω–æ–≤
        top_10 = predictions_df.head(10)
        fig1 = px.bar(
            top_10, 
            x='–ü—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–∞–∂ (—à—Ç.)', 
            y='–ë—É—Ç–∏–∫',
            orientation='h',
            title="–¢–æ–ø-10 –º–∞–≥–∞–∑–∏–Ω–æ–≤ –ø–æ –ø—Ä–æ–≥–Ω–æ–∑—É –ø—Ä–æ–¥–∞–∂",
            color='–†–µ–π—Ç–∏–Ω–≥ (%)',
            color_continuous_scale='viridis'
        )
        fig1.update_layout(height=400)
        st.plotly_chart(fig1, use_container_width=True)
    
    with col2:
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        category_counts = predictions_df['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'].value_counts()
        fig2 = px.pie(
            values=category_counts.values,
            names=category_counts.index,
            title="–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–∞–≥–∞–∑–∏–Ω–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"
        )
        fig2.update_layout(height=400)
        st.plotly_chart(fig2, use_container_width=True)
    
    # –ì—Ä–∞—Ñ–∏–∫ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    if 'feature_importance' in metrics:
        importance_df = pd.DataFrame(
            list(metrics['feature_importance'].items()),
            columns=['–ü—Ä–∏–∑–Ω–∞–∫', '–í–∞–∂–Ω–æ—Å—Ç—å']
        ).sort_values('–í–∞–∂–Ω–æ—Å—Ç—å', ascending=True)
        
        fig3 = px.bar(
            importance_df,
            x='–í–∞–∂–Ω–æ—Å—Ç—å',
            y='–ü—Ä–∏–∑–Ω–∞–∫',
            orientation='h',
            title="–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –º–æ–¥–µ–ª–∏"
        )
        fig3.update_layout(height=300)
        st.plotly_chart(fig3, use_container_width=True)

# --- –û–°–ù–û–í–ù–û–ï –ü–†–ò–õ–û–ñ–ï–ù–ò–ï ---

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Å—Å–∏–∏
if 'processed' not in st.session_state:
    st.session_state.processed = False

# –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞
dataset_file = st.file_uploader(
    "üíñ –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ –æ –ø—Ä–æ–¥–∞–∂–∞—Ö",
    type=["csv", "xlsx", "xls"],
    help="–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ñ–æ—Ä–º–∞—Ç—ã: CSV, Excel (xlsx, xls). –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: 50MB"
)

if dataset_file:
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df_raw = load_data(dataset_file)
    
    if df_raw is not None:
        st.session_state.df_raw = df_raw
        
        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ñ–∞–π–ª–µ
        with st.expander("üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º —Ñ–∞–π–ª–µ", expanded=False):
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("–°—Ç—Ä–æ–∫", len(df_raw))
            with col2:
                st.metric("–ö–æ–ª–æ–Ω–æ–∫", len(df_raw.columns))
            with col3:
                st.metric("–†–∞–∑–º–µ—Ä (MB)", f"{dataset_file.size / (1024*1024):.2f}")
            
            st.dataframe(df_raw.head(10), use_container_width=True)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–ª–æ–Ω–æ–∫
        st.subheader("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.write("**–û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏** (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ):")
            cols = st.columns(5)
            
            with cols[0]:
                art_col = st.selectbox(
                    "–ê—Ä—Ç–∏–∫—É–ª —Ç–æ–≤–∞—Ä–∞",
                    options=df_raw.columns,
                    index=safe_index_selection(df_raw.columns, 0)
                )
            
            with cols[1]:
                magazin_col = st.selectbox(
                    "–ú–∞–≥–∞–∑–∏–Ω/–ë—É—Ç–∏–∫",
                    options=df_raw.columns,
                    index=safe_index_selection(df_raw.columns, 1)
                )
            
            with cols[2]:
                date_col = st.selectbox(
                    "–î–∞—Ç–∞ –ø—Ä–æ–¥–∞–∂–∏",
                    options=df_raw.columns,
                    index=safe_index_selection(df_raw.columns, 2)
                )
            
            with cols[3]:
                qty_col = st.selectbox(
                    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ",
                    options=df_raw.columns,
                    index=safe_index_selection(df_raw.columns, 3)
                )
            
            with cols[4]:
                price_col = st.selectbox(
                    "–¶–µ–Ω–∞",
                    options=df_raw.columns,
                    index=safe_index_selection(df_raw.columns, 4)
                )
        
        with col2:
            st.write("**–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏** (–Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ):")
            
            # –í—ã–±–æ—Ä –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            required_cols = [art_col, magazin_col, date_col, qty_col, price_col]
            available_cols = [col for col in df_raw.columns if col not in required_cols]
            
            cat_features = st.multiselect(
                "–í—ã–±–µ—Ä–∏—Ç–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏",
                options=available_cols,
                help="–ù–∞–ø—Ä–∏–º–µ—Ä: —Ä–∞–∑–º–µ—Ä, —Ü–≤–µ—Ç, –∫–æ–ª–ª–µ–∫—Ü–∏—è, –±—Ä–µ–Ω–¥, —Å–µ–∑–æ–Ω"
            )
        
        # –ö–Ω–æ–ø–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        if st.button(
            "üöÄ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –∏ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å",
            type="primary",
            use_container_width=True
        ):
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            required_columns = [art_col, magazin_col, date_col, qty_col, price_col]
            
            if validate_dataframe(df_raw, required_columns):
                with st.spinner("üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ..."):
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
                    df_agg, stats = process_and_aggregate(
                        df_raw, art_col, magazin_col, date_col, 
                        qty_col, price_col, tuple(cat_features)
                    )
                    
                    if len(df_agg) > 0:
                        st.session_state.df_agg = df_agg
                        st.session_state.stats = stats
                        st.session_state.cat_features = cat_features
                        
                        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                        st.success("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!")
                        
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            st.metric("–ó–∞–ø–∏—Å–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è", stats['final_rows'])
                        with col2:
                            st.metric("–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤", stats['unique_products'])
                        with col3:
                            st.metric("–ë—É—Ç–∏–∫–æ–≤", stats['unique_stores'])
                        with col4:
                            st.metric("–û—á–∏—â–µ–Ω–æ —Å—Ç—Ä–æ–∫", stats['outliers_removed'])
                        
                        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
                        with st.spinner("üß† –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å..."):
                            model, features, metrics = train_model_with_optuna(
                                df_agg, tuple(cat_features), n_trials=30
                            )
                            
                            st.session_state.model = model
                            st.session_state.features = features
                            st.session_state.metrics = metrics
                            st.session_state.processed = True
                            
                            st.success("üéØ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞!")
                            
                            # –ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏
                            col1, col2 = st.columns(2)
                            with col1:
                                st.metric("–°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ (MAE)", f"{metrics['MAE']:.2f}")
                            with col2:
                                st.metric("–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–µ—Ç–µ—Ä–º–∏–Ω–∞—Ü–∏–∏ (R¬≤)", f"{metrics['R2']:.3f}")
                    else:
                        st.error("‚ùå –ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞!")

# –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ
if st.session_state.get('processed', False):
    st.divider()
    st.subheader("üîÆ –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–¥–∞–∂")
    
    # –§–æ—Ä–º–∞ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è
    submitted, new_product_data = create_prediction_form(
        st.session_state.cat_features,
        st.session_state.df_agg
    )
    
    if submitted:
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤
        predictions_df = make_predictions(
            st.session_state.model,
            st.session_state.features,
            new_product_data,
            st.session_state.df_agg
        )
        
        if not predictions_df.empty:
            st.success("üéâ –ü—Ä–æ–≥–Ω–æ–∑ –≥–æ—Ç–æ–≤!")
            
            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            st.subheader("üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è")
            
            # –¢–æ–ø-3 –º–∞–≥–∞–∑–∏–Ω–∞
            top_3 = predictions_df.head(3)
            
            col1, col2, col3 = st.columns(3)
            for i, (_, row) in enumerate(top_3.iterrows()):
                with [col1, col2, col3][i]:
                    st.metric(
                        f"ü•á {row['–ë—É—Ç–∏–∫']}" if i == 0 else f"ü•à {row['–ë—É—Ç–∏–∫']}" if i == 1 else f"ü•â {row['–ë—É—Ç–∏–∫']}",
                        f"{int(row['–ü—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–∞–∂ (30 –¥–Ω–µ–π, —à—Ç.)'])} —à—Ç.",
                        f"{int(row['–†–µ–π—Ç–∏–Ω–≥ —É—Å–ø–µ—Ö–∞ (%)'])}%"
                    )
            
            # –ü–æ–ª–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            st.dataframe(
                predictions_df,
                use_container_width=True,
                hide_index=True
            )
            
            # –ö–Ω–æ–ø–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
            csv = predictions_df.to_csv(index=False)
            st.download_button(
                label="üíæ –°–∫–∞—á–∞—Ç—å –ø—Ä–æ–≥–Ω–æ–∑ (CSV)",
                data=csv,
                file_name="fashion_sales_forecast.csv",
                mime="text/csv"
            )

# –°–ø—Ä–∞–≤–∫–∞
with st.sidebar:
    st.header("‚ÑπÔ∏è –°–ø—Ä–∞–≤–∫–∞")
    st.markdown("""
    **–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**
    1. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ –æ –ø—Ä–æ–¥–∞–∂–∞—Ö
    2. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–ª–æ–Ω–æ–∫
    3. –í—ã–±–µ—Ä–∏—Ç–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    4. –ù–∞–∂–º–∏—Ç–µ "–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–∞–Ω–Ω—ã–µ"
    5. –ó–∞–ø–æ–ª–Ω–∏—Ç–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Ç–æ–≤–∞—Ä–∞
    6. –ü–æ–ª—É—á–∏—Ç–µ –ø—Ä–æ–≥–Ω–æ–∑ –ø–æ –≤—Å–µ–º –±—É—Ç–∏–∫–∞–º
    
    **–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –¥–∞–Ω–Ω—ã–º:**
    - –ú–∏–Ω–∏–º—É–º 10 –∑–∞–ø–∏—Å–µ–π
    - –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: –∞—Ä—Ç–∏–∫—É–ª, –º–∞–≥–∞–∑–∏–Ω, –¥–∞—Ç–∞, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ, —Ü–µ–Ω–∞
    - –§–æ—Ä–º–∞—Ç—ã —Ñ–∞–π–ª–æ–≤: CSV, Excel
    """)
    
    if st.session_state.get('processed', False):
        st.success("‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")
        st.info(f"–¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: R¬≤ = {st.session_state.metrics['R2']:.3f}")
