import streamlit as st
import pandas as pd
import numpy as np
from catboost import CatBoostRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score
import optuna
import logging
from typing import Dict, List, Tuple, Optional
import warnings

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
warnings.filterwarnings('ignore')

# --- –°–¢–ò–õ–ò–ó–ê–¶–ò–Ø –ò–ù–¢–ï–†–§–ï–ô–°–ê ---
st.set_page_config(page_title="–ú–æ–¥–Ω—ã–π –°–æ–≤–µ—Ç–Ω–∏–∫", layout="wide", initial_sidebar_state="collapsed")

st.markdown("""
<style>
.main { background-color: #fce4ec; }
h1 { font-family: 'Comic Sans MS', cursive, sans-serif; color: #e91e63; text-align: center; text-shadow: 2px 2px 4px #f8bbd0; }
h2, h3 { font-family: 'Comic Sans MS', cursive, sans-serif; color: #ad1457; }
.stButton>button {
    color: white; background-image: linear-gradient(to right, #f06292, #e91e63); border-radius: 25px;
    border: 2px solid #ad1457; padding: 12px 28px; font-weight: bold; font-size: 18px;
    box-shadow: 0 4px 15px 0 rgba(233, 30, 99, 0.4); transition: all 0.3s ease 0s;
}
.stButton>button:hover { background-position: right center; box-shadow: 0 4px 15px 0 rgba(233, 30, 99, 0.75); }
.stExpander { border: 2px solid #f8bbd0; border-radius: 10px; background-color: #fff1f8; }
.metric-card { padding: 10px; border-radius: 10px; background-color: #fff1f8; border: 1px solid #f8bbd0; }
</style>
""", unsafe_allow_html=True)

st.title("üíñ –ú–æ–¥–Ω—ã–π –°–æ–≤–µ—Ç–Ω–∏–∫ –ø–æ –ü—Ä–æ–¥–∞–∂–∞–º üíñ")

# --- –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ---

def validate_dataframe(df: pd.DataFrame, required_columns: List[str]) -> bool:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    if df.empty:
        st.error("–§–∞–π–ª –ø—É—Å—Ç! –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏.")
        return False
    
    missing_cols = [col for col in required_columns if col not in df.columns]
    if missing_cols:
        st.error(f"–í —Ñ–∞–π–ª–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing_cols}")
        return False
    
    if len(df) < 10:
        st.warning("–°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (–º–∏–Ω–∏–º—É–º 10 –∑–∞–ø–∏—Å–µ–π)")
        return False
    
    return True

def safe_index_selection(columns: List[str], default_index: int = 0) -> int:
    """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –≤—ã–±–æ—Ä –∏–Ω–¥–µ–∫—Å–∞ –∫–æ–ª–æ–Ω–∫–∏"""
    if not columns:
        return 0
    return min(default_index, len(columns) - 1)

@st.cache_data
def load_data(file) -> Optional[pd.DataFrame]:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞ (–º–∞–∫—Å–∏–º—É–º 50MB)
        if hasattr(file, 'size') and file.size > 50 * 1024 * 1024:
            st.error("–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π! –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: 50MB")
            return None
            
        if file.name.endswith('.csv'):
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
            for encoding in ['utf-8', 'cp1251', 'latin1']:
                try:
                    df = pd.read_csv(file, encoding=encoding)
                    logger.info(f"CSV —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π {encoding}")
                    return df
                except UnicodeDecodeError:
                    continue
            st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–¥–∏—Ä–æ–≤–∫—É CSV —Ñ–∞–π–ª–∞")
            return None
        else:
            df = pd.read_excel(file, engine='openpyxl')
            logger.info("Excel —Ñ–∞–π–ª —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω")
            return df
            
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞: {str(e)}")
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞: {e}")
        return None

@st.cache_data
def process_and_aggregate(
    _df: pd.DataFrame, 
    art_col: str, 
    magazin_col: str, 
    date_col: str, 
    qty_col: str, 
    price_col: str, 
    cat_features: Tuple[str, ...]
) -> Tuple[pd.DataFrame, Dict]:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"""
    
    df = _df.copy()
    
    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫
    column_map = {
        art_col: 'Art', 
        magazin_col: 'Magazin', 
        date_col: 'date', 
        qty_col: 'Qty', 
        price_col: 'Price'
    }
    df.rename(columns=column_map, inplace=True)
    
    initial_rows = len(df)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π
    df['date'] = pd.to_datetime(df['date'], errors='coerce')
    bad_date_rows = df['date'].isna().sum()
    df.dropna(subset=['date'], inplace=True)
    
    # –û—á–∏—Å—Ç–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
    crucial_cols = ['Qty', 'Art', 'Magazin', 'Price']
    df.dropna(subset=crucial_cols, inplace=True)
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    df = df[df['Qty'] > 0]  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º
    df = df[df['Price'] > 0]  # –¶–µ–Ω–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–π
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É (–∫–≤–∞–Ω—Ç–∏–ª–∏ 1% –∏ 99%)
    qty_q1, qty_q99 = df['Qty'].quantile([0.01, 0.99])
    df = df[(df['Qty'] >= qty_q1) & (df['Qty'] <= qty_q99)]
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –ø–æ —Ü–µ–Ω–µ
    price_q1, price_q99 = df['Price'].quantile([0.01, 0.99])
    df = df[(df['Price'] >= price_q1) & (df['Price'] <= price_q99)]
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∏ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞
    df = df.sort_values(by=['Art', 'Magazin', 'date'])
    
    # –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–≤—É—é –¥–∞—Ç—É –ø—Ä–æ–¥–∞–∂–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã —Ç–æ–≤–∞—Ä-–º–∞–≥–∞–∑–∏–Ω
    first_sale_dates = df.groupby(['Art', 'Magazin'])['date'].first().reset_index()
    first_sale_dates.rename(columns={'date': 'first_sale_date'}, inplace=True)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    df_merged = pd.merge(df, first_sale_dates, on=['Art', 'Magazin'])
    
    # –ë–µ—Ä–µ–º –¥–∞–Ω–Ω—ã–µ –∑–∞ –ø–µ—Ä–≤—ã–µ 30 –¥–Ω–µ–π
    df_30_days = df_merged[
        df_merged['date'] <= (df_merged['first_sale_date'] + pd.Timedelta(days=30))
    ].copy()
    
    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    agg_logic = {'Qty': 'sum', 'Price': 'mean'}
    for cat_col in cat_features:
        if cat_col in df_30_days.columns:
            agg_logic[cat_col] = 'first'
    
    df_agg = df_30_days.groupby(['Art', 'Magazin'], as_index=False).agg(agg_logic)
    df_agg.rename(columns={'Qty': 'Qty_30_days'}, inplace=True)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    stats = {
        "total_rows": initial_rows,
        "final_rows": len(df_agg),
        "bad_date_rows": bad_date_rows,
        "outliers_removed": initial_rows - len(df_30_days),
        "unique_products": df_agg['Art'].nunique(),
        "unique_stores": df_agg['Magazin'].nunique()
    }
    
    logger.info(f"–î–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã: {initial_rows} -> {len(df_agg)} —Å—Ç—Ä–æ–∫")
    
    return df_agg, stats

@st.cache_resource
def train_model_with_optuna(
    _df_agg: pd.DataFrame, 
    cat_features: Tuple[str, ...],
    n_trials: int = 50
) -> Tuple[CatBoostRegressor, List[str], Dict]:
    """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å Optuna"""
    
    cat_features_list = list(cat_features)
    target = 'Qty_30_days'
    features = ['Magazin', 'Price'] + cat_features_list
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df_processed = _df_agg[features + [target]].copy()
    all_cat_features = ['Magazin'] + cat_features_list
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    for col in all_cat_features:
        if col in df_processed.columns:
            df_processed[col] = df_processed[col].astype(str)
    
    X, y = df_processed[features], df_processed[target]
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
    if len(X) < 50:
        st.warning("‚ö†Ô∏è –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è >50 –∑–∞–ø–∏—Å–µ–π)")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    test_size = min(0.25, max(0.1, 20 / len(X)))  # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42
    )
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    def objective(trial):
        params = {
            'iterations': trial.suggest_int('iterations', 500, 1500),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
            'depth': trial.suggest_int('depth', 4, 10),
            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),
            'border_count': trial.suggest_int('border_count', 32, 255),
            'verbose': False,
            'random_seed': 42
        }
        
        try:
            model = CatBoostRegressor(**params)
            model.fit(
                X_train, y_train, 
                cat_features=all_cat_features,
                eval_set=(X_test, y_test),
                early_stopping_rounds=50,
                use_best_model=True,
                verbose=False
            )
            predictions = model.predict(X_test)
            return mean_absolute_error(y_test, predictions)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ objective —Ñ—É–Ω–∫—Ü–∏–∏: {e}")
            return float('inf')
    
    # –û–±—É—á–µ–Ω–∏–µ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    optuna.logging.set_verbosity(optuna.logging.WARNING)
    study = optuna.create_study(direction='minimize')
    
    for i in range(n_trials):
        study.optimize(objective, n_trials=1)
        progress = (i + 1) / n_trials
        progress_bar.progress(progress)
        status_text.text(f"–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: {i+1}/{n_trials} –ø–æ–ø—ã—Ç–æ–∫")
    
    progress_bar.empty()
    status_text.empty()
    
    # –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    best_params = study.best_params
    best_params['verbose'] = False
    best_params['random_seed'] = 42
    
    final_model = CatBoostRegressor(**best_params)
    final_model.fit(X, y, cat_features=all_cat_features, verbose=False)
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    test_preds = final_model.predict(X_test)
    metrics = {
        'MAE': mean_absolute_error(y_test, test_preds),
        'R2': r2_score(y_test, test_preds),
        'best_params': best_params
    }
    
    logger.info(f"–ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞. MAE: {metrics['MAE']:.2f}, R¬≤: {metrics['R2']:.2f}")
    
    return final_model, features, metrics

def create_prediction_form(cat_features: List[str], df_agg: pd.DataFrame) -> Tuple[bool, Dict]:
    """–°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ñ–æ—Ä–º—ã –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è"""
    
    st.subheader("‚úçÔ∏è –û–ø–∏—à–∏—Ç–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏")
    
    with st.form("prediction_form"):
        new_product_data = {}
        
        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫
        n_features = len(cat_features)
        n_cols = min(3, max(1, n_features))
        cols = st.columns(n_cols)
        
        # –ü–æ–ª—è –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        for i, feature in enumerate(cat_features):
            with cols[i % n_cols]:
                if feature in df_agg.columns:
                    # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è –ø–æ–¥—Å–∫–∞–∑–∫–∏
                    top_values = df_agg[feature].value_counts().head(5).index.tolist()
                    help_text = f"–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ: {', '.join(map(str, top_values[:3]))}"
                    placeholder = f"–ù–∞–ø—Ä–∏–º–µ—Ä: {top_values[0]}" if top_values else "–í–≤–µ–¥–∏—Ç–µ –∑–Ω–∞—á–µ–Ω–∏–µ"
                    
                    new_product_data[feature] = st.text_input(
                        f"{feature} ‚ú®",
                        help=help_text,
                        placeholder=placeholder
                    )
                else:
                    new_product_data[feature] = st.text_input(f"{feature} ‚ú®")
        
        # –ü–æ–ª–µ –¥–ª—è —Ü–µ–Ω—ã
        if 'Price' in df_agg.columns:
            price_stats = df_agg['Price'].describe()
            price_mean = float(price_stats['mean'])
            price_min = float(price_stats['min'])
            price_max = float(price_stats['max'])
            
            with cols[n_features % n_cols]:
                new_product_data['Price'] = st.number_input(
                    "–¶–µ–Ω–∞ üí∞",
                    min_value=0.0,
                    value=price_mean,
                    step=max(1.0, price_mean * 0.05),
                    format="%.2f",
                    help=f"–î–∏–∞–ø–∞–∑–æ–Ω —Ü–µ–Ω –≤ –¥–∞–Ω–Ω—ã—Ö: {price_min:.0f} - {price_max:.0f}"
                )
        
        submitted = st.form_submit_button("üîÆ –°–æ–∑–¥–∞—Ç—å –ø—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–∞–∂!")
        
        return submitted, new_product_data

def make_predictions(
    model: CatBoostRegressor, 
    features: List[str], 
    new_product_data: Dict, 
    df_agg: pd.DataFrame
) -> pd.DataFrame:
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –¥–ª—è –≤—Å–µ—Ö –º–∞–≥–∞–∑–∏–Ω–æ–≤"""
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    for key, value in new_product_data.items():
        if key != 'Price' and (pd.isna(value) or str(value).strip() == ""):
            st.error(f"‚ö†Ô∏è –ü–æ–ª–µ '{key}' –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º!")
            return pd.DataFrame()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞
    magaziny = df_agg['Magazin'].unique()
    predictions_data = []
    
    for magazin in magaziny:
        row = new_product_data.copy()
        row['Magazin'] = magazin
        predictions_data.append(row)
    
    predictions_df = pd.DataFrame(predictions_data)[features]
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤
    try:
        raw_predictions = model.predict(predictions_df)
        predictions_df['Pred_Qty_30_days'] = np.maximum(0, np.round(raw_predictions, 0))
        
        # –†–∞—Å—á–µ—Ç —Ä–µ–π—Ç–∏–Ω–≥–∞
        max_pred = predictions_df['Pred_Qty_30_days'].max()
        if max_pred > 0:
            predictions_df['Rating_%'] = np.round(
                (predictions_df['Pred_Qty_30_days'] / max_pred * 100), 0
            )
        else:
            predictions_df['Rating_%'] = 0
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é –ø—Ä–æ–≥–Ω–æ–∑–∞
        result_df = predictions_df.sort_values(
            by='Pred_Qty_30_days', ascending=False
        ).rename(columns={
            'Magazin': '–ë—É—Ç–∏–∫',
            'Pred_Qty_30_days': '–ü—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–∞–∂ (30 –¥–Ω–µ–π, —à—Ç.)',
            'Rating_%': '–†–µ–π—Ç–∏–Ω–≥ —É—Å–ø–µ—Ö–∞ (%)'
        })
        
        return result_df
        
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –ø—Ä–æ–≥–Ω–æ–∑–∞: {str(e)}")
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        return pd.DataFrame()

# --- –û–°–ù–û–í–ù–û–ï –ü–†–ò–õ–û–ñ–ï–ù–ò–ï ---

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Å—Å–∏–∏
if 'processed' not in st.session_state:
    st.session_state.processed = False

# –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞
dataset_file = st.file_uploader(
    "üíñ –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ –æ –ø—Ä–æ–¥–∞–∂–∞—Ö",
    type=["csv", "xlsx", "xls"],
    help="–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ñ–æ—Ä–º–∞—Ç—ã: CSV, Excel (xlsx, xls). –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: 50MB"
)

if dataset_file:
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df_raw = load_data(dataset_file)
    
    if df_raw is not None:
        st.session_state.df_raw = df_raw
        
        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ñ–∞–π–ª–µ
        with st.expander("üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º —Ñ–∞–π–ª–µ", expanded=False):
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("–°—Ç—Ä–æ–∫", len(df_raw))
            with col2:
                st.metric("–ö–æ–ª–æ–Ω–æ–∫", len(df_raw.columns))
            with col3:
                st.metric("–†–∞–∑–º–µ—Ä (MB)", f"{dataset_file.size / (1024*1024) }
